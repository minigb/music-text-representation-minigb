{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00de35c6",
   "metadata": {},
   "source": [
    "# Demo for Toward Universal Text-to-Music Retrieval\n",
    "\n",
    "- arXiv: https://arxiv.org/abs/2211.14558\n",
    "- pretrained model: https://zenodo.org/record/7322135\n",
    "- github repo: https://github.com/seungheondoh/music-text-representation\n",
    "- demo site: https://seungheondoh.github.io/text-music-representation-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a3aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio, HTML\n",
    "\n",
    "import argparse\n",
    "from mtr.utils.demo_utils import get_model\n",
    "from mtr.utils.eval_utils import _text_representation\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0932134e",
   "metadata": {},
   "source": [
    "## Load Pretrained Model & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2782b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check https://github.com/seungheondoh/msd-subsets\n",
    "# your_msd_path = \"dataset\"\n",
    "# msd_path = os.path.join(your_msd_path, \"msd-subsets/dataset\")\n",
    "msd_path = '/home/minhee/userdata/music-text-representation-minigb/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea3bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "global msd_to_id\n",
    "global id_to_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e6eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_to_id = pickle.load(open(os.path.join(msd_path, \"lastfm_annotation\", \"MSD_id_to_7D_id.pkl\"), 'rb'))\n",
    "id_to_path = pickle.load(open(os.path.join(msd_path, \"lastfm_annotation\", \"7D_id_to_path.pkl\"), 'rb'))\n",
    "annotation = json.load(open(os.path.join(msd_path, \"ecals_annotation/annotation.json\"), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf981c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_extract_audio_embedding(framework, text_type, text_rep):\n",
    "    ecals_test = torch.load(f\"../mtr/{framework}/exp/transformer_cnn_cf_mel/{text_type}_{text_rep}/audio_embs.pt\")\n",
    "    msdid = [k for k in ecals_test.keys()]\n",
    "    audio_embs = [ecals_test[k] for k in msdid]\n",
    "    audio_embs = torch.stack(audio_embs)\n",
    "    return audio_embs, msdid\n",
    "\n",
    "def model_load(framework, text_type, text_rep):\n",
    "    audio_embs, msdid = pre_extract_audio_embedding(framework, text_type, text_rep)\n",
    "    model, tokenizer, config = get_model(framework=framework, text_type=text_type, text_rep=text_rep)\n",
    "    return model, audio_embs, tokenizer, msdid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023f8f3",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e3465dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_fn(query, tokenizer, model, audio_embs, msdid, annotation):\n",
    "    text_input = tokenizer(query, return_tensors=\"pt\")['input_ids']\n",
    "    with torch.no_grad():\n",
    "        text_embs = model.encode_bert_text(text_input, None)\n",
    "    audio_embs = nn.functional.normalize(audio_embs, dim=1)\n",
    "    text_embs = nn.functional.normalize(text_embs, dim=1)\n",
    "    logits = text_embs @ audio_embs.T\n",
    "    ret_item = pd.Series(logits.squeeze(0).numpy(), index=msdid)\n",
    "    instance = {}\n",
    "    metadata = {}\n",
    "    for idx, _id in enumerate(ret_item.sort_values(ascending=False).head(3).index):\n",
    "        music_obj = ipd.Audio(os.path.join(msd_path, 'songs', id_to_path[msd_to_id[_id]]) , rate=22050)\n",
    "        meta_obj = annotation[_id]\n",
    "        metadata[f'top{idx+1} music'] = meta_obj['tag']\n",
    "        music_src = music_obj.src_attr()\n",
    "        instance[f'top{idx+1} music'] = f\"\"\"<audio controls><source src=\"{music_src}\" type=\"audio/wav\"></audio></td>\"\"\"\n",
    "    return instance, metadata\n",
    "\n",
    "def retrieval_show(framework, text_type, text_rep, annotation, query, is_audio=True):    \n",
    "    model, audio_embs, tokenizer, msdid = model_load(framework, text_type, text_rep)\n",
    "    meta_results, retrieval_results = [], []\n",
    "    for i in query:\n",
    "        instance, metadata = retrieval_fn(i, tokenizer, model, audio_embs, msdid, annotation)\n",
    "        retrieval_results.append(instance)\n",
    "        meta_results.append(metadata)\n",
    "    if is_audio:\n",
    "        inference = pd.DataFrame(retrieval_results, index=query)\n",
    "        html = inference.to_html(escape=False)\n",
    "    else:\n",
    "        inference = pd.DataFrame(meta_results, index=query)\n",
    "        html = inference.to_html(escape=False)\n",
    "    ipd.display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bada436",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_query = \"banjo\"\n",
    "# caption_query = \"fusion jazz with synth, bass, drums, saxophone\"\n",
    "# unseen_query = \"music for meditation or listen to in the forest\"\n",
    "query = [tag_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134e6c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework='classification' # triplet\n",
    "text_type='bert' # tag, caption\n",
    "text_rep=\"stochastic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b8c48b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../mtr/classification/exp/transformer_cnn_cf_mel/bert_stochastic/audio_embs.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretrieval_show\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_audio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mretrieval_show\u001b[0;34m(framework, text_type, text_rep, annotation, query, is_audio)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieval_show\u001b[39m(framework, text_type, text_rep, annotation, query, is_audio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):    \n\u001b[0;32m---> 20\u001b[0m     model, audio_embs, tokenizer, msdid \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_rep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     meta_results, retrieval_results \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m query:\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mmodel_load\u001b[0;34m(framework, text_type, text_rep)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_load\u001b[39m(framework, text_type, text_rep):\n\u001b[0;32m----> 9\u001b[0m     audio_embs, msdid \u001b[38;5;241m=\u001b[39m \u001b[43mpre_extract_audio_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_rep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     model, tokenizer, config \u001b[38;5;241m=\u001b[39m get_model(framework\u001b[38;5;241m=\u001b[39mframework, text_type\u001b[38;5;241m=\u001b[39mtext_type, text_rep\u001b[38;5;241m=\u001b[39mtext_rep)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, audio_embs, tokenizer, msdid\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36mpre_extract_audio_embedding\u001b[0;34m(framework, text_type, text_rep)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpre_extract_audio_embedding\u001b[39m(framework, text_type, text_rep):\n\u001b[0;32m----> 2\u001b[0m     ecals_test \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../mtr/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mframework\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/exp/transformer_cnn_cf_mel/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtext_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtext_rep\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/audio_embs.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     msdid \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ecals_test\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m      4\u001b[0m     audio_embs \u001b[38;5;241m=\u001b[39m [ecals_test[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m msdid]\n",
      "File \u001b[0;32m~/miniconda3/envs/ttmr_seungheon/lib/python3.8/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/ttmr_seungheon/lib/python3.8/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/ttmr_seungheon/lib/python3.8/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../mtr/classification/exp/transformer_cnn_cf_mel/bert_stochastic/audio_embs.pt'"
     ]
    }
   ],
   "source": [
    "retrieval_show(framework, text_type, text_rep, annotation, query, is_audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adf9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
